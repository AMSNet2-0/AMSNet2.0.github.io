<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="static/css/index.css">

  <style>
    #workflow-carousel .item {
      /* 让每个轮播项目的高度由其内容自然决定，而不是强制固定 */
      height: auto;
    }
  
    #workflow-carousel .card {
      /* 确保卡片作为一个整体，高度也能自适应 */
      display: flex;
      flex-direction: column;
      height: 100%;
    }
  
    #workflow-carousel .card-image img {
      /* 设置一个最大高度，防止图片过大，但允许它根据自身比例缩放 */
      max-height: 350px; 
      width: auto; /* 允许宽度自动调整以保持比例 */
      margin: 0 auto; /* 让图片在容器内水平居中 */
      display: block;
    }
  
    #workflow-carousel .card-content {
      /* 让文字内容区域可以自由伸展 */
      flex-grow: 1;
    }
  </style>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AMSnet 2.0: A Large AMS Databasenwith AI Segmentation for Net Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=m5NipB4AAAAJ&hl=en" target="_blank">Yichen Shi</a><sup>1</sup>*,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=MszI2TAAAAAJ&hl=zh-CN&oi=ao" target="_blank">Zhuofu Tao</a><sup>2</sup>*,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=5biRFwUAAAAJ" target="_blank">Yuhao Gao</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="" target="_blank">Li Huang</a><sup>1</sup>*,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?hl=zh-CN&user=UDzaFPIAAAAJ" target="_blank">Hongyang Wang</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="" target="_blank">Zhiping Yu</a><sup>3</sup>,</span>
                          <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=P8suglMAAAAJ&hl=zh-CN&oi=ao" target="_blank">Ting-Jung Lin</a><sup>1</sup>**,</span>
                          <span class="author-block">
                            <a href="https://scholar.google.com/citations?hl=zh-CN&user=n_N-PJkAAAAJ" target="_blank">Lei He</a><sup>1</sup>**,</span>
                                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China</span>
                    <span class="author-block"><sup>2</sup>University of California, Los Angeles, USA</span>
                    <span class="author-block"><sup>3</sup>Tsinghua University, Beijing, China</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>**</sup>Indicates Corresponding authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2505.09155" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                      
                    <!-- hugging face link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">🤗</span>
                      <span>Dataset</span>
                    </a>
                  </span>
                    

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.09155" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body" style="padding: 0;">
    <figure class="image is-fullwidth">
      <img src="static/images/AMSBench_banner.png" alt="AMSbench Teaser"
           style="width: 60%; height: auto; display: block; margin: 0 auto;">
    </figure>
    <h2 class="subtitle has-text-centered mt-4">
      AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection
    </h2>
  </div>
</section>





<!-- 摘要摘要 -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Abstract</h2>
    <div class="content">
      <p>
        Multimodal large language models (MLLM) struggle to understand circuit schematics due to their
         limited recognition capabilities. This could be attributed to the lack of high
        quality schematic-netlist training data. Existing work such as
        AMSnet applies schematic parsing to generate netlists. However,
        these methods rely on hard-coded heuristics and are difficult to
        apply to complex or noisy schematics in this paper. We therefore
        propose a novel net detection mechanism based on segmentation
        with high robustness. The proposed method also recovers positional information, allowing digital reconstruction of schematics.
        We then expand the AMSnet dataset with schematic images from
        various sources and create AMSnet 2.0. AMSnet 2.0 contains
        2,686 circuits with schematic images, Spectre-formatted netlists,
        OpenAccess digital schematics, and positional information for
        circuit components and nets, whereas AMSnet only includes 792
        circuits with SPICE netlists but no digital schematics.
      </p>
    </div>
  </div>
</section>

<section class="section" id="Workflow">
  <div class="container is-max-desktop">
      <h2 class="title is-3">Our Workflow</h2>
      
      <div class="content has-text-centered">
          <p class="has-text-justified">
              Our core contribution is the development of an end-to-end automated pipeline. It takes schematic images as input and, through a series of AI-based parsing and processing steps, transforms them into machine-readable netlists and human-editable digital schematics. The diagram below provides a high-level overview of this process.
          </p>
          <figure class="image is-4by2">
              <img src="static/images/fig1_platform.png" alt="Overall workflow of AMSnet 2.0">
          </figure>
          <p class="mt-2 is-size-6 has-text-grey">
            Figure 1: Overview of the AMSnet 2.0 dataset construction and automated generation pipeline.
          </p>
      </div>

      <div class="content has-text-centered" style="margin-top: 3rem;">
          <h3 class="title is-4">Step-by-Step Pipeline Details</h3>
          <div id="workflow-carousel" class="carousel results-carousel">
              
              <div class="item">
                  <div class="card">
                      <div class="card-image">
                          <figure class="image is-2by1">
                              <img src="static/images/workflow-step1-labeling.png" alt="Schematic Labeling">
                          </figure>
                      </div>
                      <div class="card-content">
                          <div class="media">
                              <div class="media-content">
                                  <p class="title is-4">Step 1: Data Annotation & Collection</p>
                                  <p class="subtitle is-6">Manual Input & Expert Knowledge</p>
                              </div>
                          </div>
                          <div class="content has-text-left">
                              The process begins with high-quality data annotation. We developed a labeling platform where users can upload schematics, draw bounding boxes to label components (e.g., PMOS, NMOS), and trace all individual electrical nets. Furthermore, experts can label circuit functions, topologies, and other insights, providing invaluable data for training AI models. 
                          </div>
                      </div>
                  </div>
              </div>

              <div class="item">
                  <div class="card">
                      <div class="card-image">
                          <figure class="image is-2by1">
                              <img src="static/images/workflow-step2-detection.png" alt="Device and Intersection Detection">
                          </figure>
                      </div>
                      <div class="card-content">
                          <div class="media">
                              <div class="media-content">
                                  <p class="title is-4">Step 2: Component & Intersection Detection</p>
                                  <p class="subtitle is-6">Application of Object Detection Models</p>
                              </div>
                          </div>
                          <div class="content has-text-left">
                              We use a state-of-the-art object detection model (YOLOv11) to automatically identify all components (transistors, resistors, etc.) in the input schematic.  Unlike previous work, our approach also specifically detects wire intersections, recognizing whether it is an electrical junction (with a dot) or merely a visual crossover (no dot), which is critical for correct circuit parsing. 
                          </div>
                      </div>
                  </div>
              </div>

              <div class="item">
                  <div class="card">
                      <div class="card-image">
                          <figure class="image is-2by1">
                              <img src="static/images/net-detection-method.png" alt="Wire Segmentation">
                          </figure>
                      </div>
                      <div class="card-content">
                          <div class="media">
                              <div class="media-content">
                                  <p class="title is-4">Step 3: Wire Segmentation</p>
                                  <p class="subtitle is-6">Core Two-Stage Technique</p>
                              </div>
                          </div>
                          <div class="content has-text-left">
                              This is the core innovation of our pipeline. We employ a two-stage approach: first, a U-Net semantic segmentation network extracts all wire pixels from the complex background, creating a "wire mask".  Then, a set of post-processing algorithms intelligently splits and merges this mask at intersection points to precisely separate the different electrical nets. 
                          </div>
                      </div>
                  </div>
              </div>

              <div class="item">
                  <div class="card">
                      <div class="card-image">
                          <figure class="image is-2by1">
                              <img src="static/images/workflow-step4-reconstruction.png" alt="Netlist and Schematic Generation">
                          </figure>
                      </div>
                      <div class="card-content">
                          <div class="media">
                              <div class="media-content">
                                  <p class="title is-4">Step 4: Netlist & Schematic Generation</p>
                                  <p class="subtitle is-6">Final Output Generation</p>
                              </div>
                          </div>
                          <div class="content has-text-left">
                              After identifying all components, their pin locations, and the individual electrical nets, our tool automatically analyzes their connectivity. By determining which net intersects with or is nearest to each component pin, it establishes the complete circuit topology and generates an accurate Spectre-formatted netlist.  Simultaneously, using the coordinate information, the system reconstructs an editable digital schematic in formats like OpenAccess. 
                          </div>
                      </div>
                  </div>
              </div>

          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motivation: The Challenge for AI in Circuit Understanding</h2>
        <div class="content has-text-justified">
          <p>
            Although Multimodal Large Language Models (MLLMs) have shown powerful capabilities in many fields, they struggle to understand and parse professional circuit schematics.  This is largely due to the lack of high-quality, large-scale "schematic-netlist" paired data for training.  Existing methods for schematic parsing, such as those used in AMSnet 1.0, often rely on hard-coded heuristics, making them difficult to apply to complex or noisy real-world schematics. 
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="static/images/noisy-schematics.png" alt="Noisy schematics with markings and highlights" style="width: 70%;"/>
            <p class="mt-2 is-size-6 has-text-grey">
              Fig. 2: Examples of noisy schematics with overlaid markings (a) and partial highlighting (b), which are challenging for traditional methods. 
            </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered" style="margin-top: 3rem;">
      <div class="column is-full-width">
        <h2 class="title is-3">Our Approach: A Robust Segmentation-Based Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            To overcome the limitations of traditional methods, we propose a novel, deep-learning-based pipeline for schematic parsing. This pipeline not only generates netlists but also recovers positional information to reconstruct editable digital schematics. 
          </p>
        </div>

        <div class="content has-text-centered" style="margin-top: 2rem;">
            <h3 class="title is-4">Core Technique: Two-Stage Net Detection</h3>
            <p class="has-text-justified">
              The core of our method is an innovative two-stage approach for net detection, which significantly improves the accuracy and robustness of identifying wires from complex images. In the first stage, we use a semantic segmentation network (U-Net) to produce a "mask" of all wire pixels, separating them from the background and other elements.  In the second stage, we post-process this wire mask, especially at intersection points, using intelligent split-and-merge operations to precisely delineate wire segments belonging to different nets.  This approach eliminates the need for bounding box proposals, fundamentally improving segmentation accuracy. 
            </p>
            <img src="static/images/net-detection-method.png" alt="Two-stage net detection method" style="width: 90%; margin-top: 1rem;"/>
            <p class="mt-2 is-size-6 has-text-grey">
              Fig. 4: Our proposed two-stage net detection method. (a) Semantic segmentation to extract all wires. (b) Split and merge operations at intersection points to identify individual nets. 
            </p>
        </div>

        <div class="content has-text-centered" style="margin-top: 2rem;">
            <h3 class="title is-4">Handling Noise with Data Augmentation</h3>
            <p class="has-text-justified">
                To enable our model to handle real-world schematics with annotations or markings, we employ a data augmentation technique using mocked markings. During training, we randomly generate rectangles and text markings over clean schematics while keeping the net mask labels unchanged.  After this augmented training, our model can successfully "see through" these mocked markings and accurately detect the underlying wires, greatly enhancing its robustness in practical application scenarios. 
            </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered" style="margin-top: 3rem;">
        <div class="column is-full-width">
          <h2 class="title is-3">The AMSnet 2.0 Dataset</h2>
          <div class="content has-text-justified">
            <p>
                We construct and release AMSnet 2.0, a large-scale, multimodal dataset for analog and mixed-signal circuits.  It contains 2,686 circuits, providing schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for all components and net segments.  In comparison, the original AMSnet only included 792 circuits with SPICE netlists and no digital schematics.  This dataset will provide a solid foundation for training more capable AI models for circuit understanding.
            </p>
          </div>
          <div class="columns">
            <div class="column">
              <div class="content has-text-centered">
                  <img src="static/images/distribution-complexity.png" alt="AMSnet 2.0 Complexity Distribution"/>
                  <p class="mt-2 is-size-6 has-text-grey">
                    Fig. 8: Distribution of schematic complexity by the number of elements and nets. 
                  </p>
              </div>
            </div>
            <div class="column">
              <div class="content has-text-centered">
                  <img src="static/images/distribution-elements.png" alt="AMSnet 2.0 Element Type Distribution"/>
                  <p class="mt-2 is-size-6 has-text-grey">
                    Fig. 9: Distribution of schematic element types in the dataset.
                  </p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="columns is-centered" style="margin-top: 3rem;">
        <div class="column is-full-width">
          <h2 class="title is-3">Experimental Results</h2>
          <div class="content has-text-justified">
            <p>
                We evaluated our method on a test set containing 80 easy, 70 medium, and 50 hard schematics.  We use the F1 score as the evaluation metric and compare our U-Net-based approach against the YOLOv11-seg model, which performs direct instance segmentation.  As shown in the table below, our method achieves significantly better performance across all difficulty levels.
            </p>
          </div>
          <div class="content">
            <h4 class="title is-4 has-text-centered">Netlist Generation F1 Score (%)</h4>
            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th class="has-text-centered">Model</th>
                  <th class="has-text-centered">Easy</th>
                  <th class="has-text-centered">Medium</th>
                  <th class="has-text-centered">Hard</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="has-text-centered"><strong>U-Net (Ours)</strong> </td>
                  <td class="has-text-centered"><strong>90.19</strong> </td>
                  <td class="has-text-centered"><strong>84.17</strong> </td>
                  <td class="has-text-centered"><strong>80.39</strong> </td>
                </tr>
                <tr>
                  <td class="has-text-centered">YOLOv11-seg </td>
                  <td class="has-text-centered">83.62 </td>
                  <td class="has-text-centered">71.10 </td>
                  <td class="has-text-centered">73.11 </td>
                </tr>
              </tbody>
            </table>
            <p class="mt-2 is-size-6 has-text-centered has-text-grey">
                Table I: F1 score comparison for the netlist generation task. 
            </p>
          </div>
          <div class="content has-text-centered" style="margin-top: 2rem;">
              <img src="static/images/results-examples.png" alt="AMSnet 2.0 Results on Easy, Medium, and Hard splits"/>
              <p class="mt-2 is-size-6 has-text-grey">
                Fig. 5: Results for element detection, net detection, and Spectre netlist generation from easy (a), medium (b), and hard (c) splits. 
              </p>
          </div>
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/reconstruction-results.png" alt="Reconstructed Schematics in OpenAccess format"/>
            <p class="mt-2 is-size-6 has-text-grey">
              Fig. 6: The digital schematics automatically reconstructed by our flow and displayed in Cadence Virtuoso, corresponding to the examples in Fig. 5. 
            </p>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/AMSnet2_0+(2).pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
